%
\begin{isabellebody}%
\setisabellecontext{thesis{\isacharunderscore}{\isadigit{5}}{\isacharunderscore}discussion}%
%
\isadelimtheory
%
\endisadelimtheory
%
\isatagtheory
%
\endisatagtheory
{\isafoldtheory}%
%
\isadelimtheory
%
\endisadelimtheory
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsection{Discussion\label{discussion}%
}
\isamarkuptrue%
%
\isamarkupsubsection{Automated Moral Agents in Practice \label{aiethics}%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
In Chapter \ref{applications}, I demonstrated that my system is capable of performing sophisticated,
nuanced ethical reasoning. In this section and Section \label{computationalethics}, I outline how my
system could be used by AI agents and by human philosophers. My work on automating the categorical imperative 
could serve as one component of a partially or fully artificial ethical reasoner, or an ``ethical AI.''
Specifically, my system could be a categorical imperative library that takes as input the logical representation 
of a maxim and returns its moral status (if it is obligatory, prohibited, or permissible).

As it stands, my project can evaluate the moral status of maxims represented in my logic and potentially 
serves as one component of an ``ethics engine'' that an AI agent could use to make ethical decisions.
For example, my system could be combined with an input parser to translate moral dilemmas as represented 
to the AI agent into maxims in my logic. The ouput of my system could be fed into an output 
parser to translate this output into a prescription for the action the AI agent should take.
Figure \ref{fig:AIengine} depicts the workflow of this example ethics engine.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{figure}
\centering
\includegraphics[scale=0.4]{AI_engine.png}
\caption{An example of an ethics engine for an artificial agent, which begins with a moral dilemma, 
passes it through an input parser, applies the automated Kantian ethics test, and then processes the 
output using an output parser. I contribute the automated Kantian ethics component.} \label{fig:AIengine}
\end{figure}
%
\begin{isamarkuptext}%
In this workflow, an AI agent is faced with a moral dilemma in some internal representation. The input
parser translates this internal representation into an appropriate logical representation, i.e. 
a circumstance, act, goal tuple. The output parser translates the output of the categorical imperative
library (the moral status of the maxim as obligatory, prohibited, or permissible) to a prescription for
action that the AI agent can act on. In order for my system to be used in an AI agent using the workflow
above, future work would need to develop such input and output parsers.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
The input parser must translate a complex real-world situation into a flat, logical representation. 
This requires that the input parser determine which circumstances are ``morally relevant''
for a maxim, a controversial judgement that requires commonsense reasoning.
There is robust debate in the literature on which circumstances should be considered when formulating a maxim. 
Some critics of Kant raise the ``tailoring objection,'' which is the worry that arbitrarily specific 
circumstances render any maxim universalizable. For example, the maxim ``When my name is Lavanya Singh 
and I am wearing a purple shirt and it is November 26th, I will lie in order to get some easy cash'' 
passes the universalizability test. Even if this maxim is willed universally, the circumstances are so 
specific that lying will not become the general mechanism for getting easy cash, so the lender will 
believe my lie and the maxim will remain effective. By tailoring the circumstances, any maxim can 
evade universalization.

The Kantian response to this criticism is to require that the circumstances included in the formulation
of the maxim be morally relevant. In the example above, my purple shirt and the date clearly have no bearing on 
the moral status of lying. On the other hand, consider the maxim, ``When I am unemployed, I will murder
someone in order to take their job.'' The circumstances of being unemployed clearly have some bearing on the moral
relevance of the murder in question; they speak to the motivation for the murder. While this view 
tracks how we actually perform moral reasoning, it raises the question of how we can determine
which circumstances are morally relevant. O'Niell answers this question by noting that the Formula of Universal Law is 
a ``test of moral worth rather than of outward rightness'' \citep[98]{constofreason}. The FUL is a way 
for an agent to decide how they should behave, not for a third-party to judge their behavior. Ethics is 
a personal process for Kant, so the FUL is designed to help agents internally make decisions, not to 
judge others' decisions. Because agents use the FUL to evaluate their own behavior, the test is at its 
best when they make a good faith effort to isolate the \emph{principle} of their action, rather than some
``surface intent'' \citep[87]{constofreason}. The FUL is supposed to determine if an agent's principle of action
is universally consistent, so it is most effective when an agent accurately formulates the principle
they act on. Circumstances are morally relevant if they accurately reflect the way that the agent is 
thinking about their own action. In the example above, the circumstance of wearing a purple shirt doesn't reflect
the principle of the liar's action. Its inclusion is clearly a disingenous attempt to evade the universalizability
test, but because the FUL is a test of personal integrity, it cannot withstand this kind of mental
gymnastics.

While this account of the formulation of a maxim describes how a well-intentioned human agent can determine 
morally relevant circumstances, the challenge remains open for automated ethics. In order for an automated
ethical agent to use the categorical imperative to its fullest extent, the input maxim fed into
my system or any automation of the FUL must be a good-faith attempt to capture the agent's principle
of action. However an action is turned into a maxim for my system to process, whether manually as I did
in Chapter \ref{applications} or using an automatic input parser, this transformation must be a good-faith 
attempt to capture the principle of action. 

Translating everyday situations into appropriate maxims is the bulk of the work that a Kantian human 
being does when making decisions. Common misconceptions about Kantian ethics\footnote{For example, critics 
of Kantian ethics worry that the maxim, ``When I am a
man, I will marry a man because I want to spend my life with him'' fails the universalizability
test because if all men only married men, sexual reproduction would stop. This argument implies 
that Kantian ethics is homophobic. Kantians often respond by arguing that the correct formulation of 
this maxim is, ``When I love a man, I will marry him because I want to spend my life with him,'' which
is universalizable because if everyone marries who they love, some men will marry women and others will
marry men.} often result from incorrectly formulated maxims, 
and the entire field of applied Kantian ethics is devoted to generating the right kinds of maxims to test. 

This representational question is one of the biggest challenges to using my categorical imperative library
in an AI ethics engine. One solution is for a human being to perform the role of the input
parser. Once an AI agent stumbles onto an ethical dilemma, a human being could take over, formulate 
the right question, and feed it into the categorical imperative library to see what action the categorical 
imperative would prescribe. For proponents of the ``human-in-the-loop'' model of AI ethics, in which 
ethical AI requires that humans guide machines \cite{loop}, this kind of human involvement may be a feature.
The outcome of the universalizability test will depend on how the human formulates the maxim; if the 
human puts garbage into the test, the test will return garbage out.

It is likely that, regardless of the strengths of the human-in-the-loop model, fully automated AI 
agents will exist. Even if developing this kind of AI is irresponsible,
such developments are likely and will require ethics engines, or risk no consideration of ethics at all. Even if 
AI without human supervision is scary, such AI with automated ethics is better than such AI without. 
In such a world, the input parser in my ethics engine would have to be automated. This would require 
that the parser translate the AI agent's internal representation to the appropriate logical representation. 
It is likely that, just as different implementations of automated ethics choose 
a particular ethical theory and implement it, different implementations of such an input parser would 
adopt different interpretations of commonsense reasoning and morally relevant circumstances. 

These interpretations of commonsense reasonining could inspire heuristics to classify circumstances as morally 
relevant. For example, one such attempt could define a moral closeness relation between an action, a 
goal, and circumstances. This heuristic could define morally relevant circumstances as those that 
reach a certain closeness threshhold with the action and the goal. Another possible heuristic could 
define some set of morally important entities, and classify morally relevant circumstances as those
that involve morally important entities. I discuss a potential machine-learning based approach which formulates
maxims based on a training set of appropriately formulated maxims in Section \ref{amapossible}.
Determining morally relevant circumstances, either using heuristics or human involvement, is a ripe 
area for future work.

Once the input has been parsed, either by a human or a machine, into  a sentence in my logic, my 
project can evaluate its moral status using my implementation of 
the FUL. Concretely, my project returns a value indicating if the maxim is obligatory, permissible, 
or prohibited. The maxim is prohibited if it fails the universalizability test, permissible if it passes, and obligatory 
if its negation fails the universalizability test. All three of these properties require testing if a 
certain theorem holds or not in my logic, a calculation that I demonstrate in Section \ref{testing}. 
Testing these properties requires that my system have a database of common sense or factual background. 
Different applications of my system may require different factual background (e.g. a self-driving car 
would need to know traffic regulations), so this common sense database may need to be application 
specific. As demonstrated in the examples in Chapter \ref{applications}, my system can produce sophisticated 
judgements with relatively little situational context. While the need for factual background is a challenge
for automated ethics, Chapter \ref{applications} demonstrates that it is less daunting than it seems. 

My system's output could be converted into some actionable, useful response with another output parser, 
and then passed back to the AI agent. For example, if the AI agent is equipped to evaluate natural 
language prescriptions, the status of the maxim could be parsed into a natural language sentence. The 
input parser, categorical imperative library, and output parser together constitute an ``ethics engine'' 
that AI developers could use as a black box implementation of an ethical theory. 

The ethics engine depicted above is a high-level example of one way to use my project to guide an artifical agent.
An automated version of the categorical imperative could become part of an ethics engine 
for an AI agent, with additional work to parse the input and the output. Effectively, the kind 
of automated ethics I implement could be a library that AI developers use to give AI agents the capacity for 
sophisticated ethical reasoning faithful to philosophical literature. This represents an improvement 
over existing AI ethics, which rarely attempts to capture the complexity 
of any ethical theory that philosophers plausibly defend.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsection{Computational Ethics \label{computationalethics}%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
Above I explained how my system offers a mechanism for humans to build ethical AI agents. I also 
argue that computational ethics is a mechanism for computers to help humans think differently about 
philosophy. Just as theorem provers make mathematics more efficient and push mathematicians to think 
precisely about the phenomena they model, computational ethics can help philosophers think more precisely about 
philosophy. Below I share an example of the kind of philosophical insight that computational ethics 
can prompt and analyze the value that this tool offers to philosophers.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsubsection{Example of a Philosophical Insight: Well-Formed Maxims%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
As presented in Section \ref{formalizingful}, in the process of developing my formalization of
the FUL, I discovered the certain kinds of maxims are badly formed, or inappropriate inputs to the 
universalizability test. The FUL is consistent only if it holds for ``well-formed maxims,''
such that neither the act nor goal are already achieved in the given circumstances. Precisely, 
a circumstance, act, goal tuple (c, a, g) is well-formed if $(\neg (c \longrightarrow a) ) \wedge 
(\neg(c \longrightarrow g))$. 

\noindent \textbf{Philosophical Implications of Badly-Formed Maxims}

Isabelle showed that the FUL can only hold for well-formed maxims logically, and I returned to Kantian
literature to better understand the philosophical implications of this idea. Because badly-formed maxims
neither change an agent's behavior nor generate meaningful obligations, they are not the right kinds of 
actions for practical reasoners to wonder about. They cannot be action-guiding and are thus not the kind of problem that 
ethics should be concerned with. Moreover, under the Kantian account of the will, the very act of asking 
if a badly-formed maxim is prohibited generates a contradiction by undermining the will's authority over itself. 

Consider the example badly-formed maxim, ``When eating breakfast, I will eat breakfast in order to 
eat breakfast.'' This maxim isn't clearly obligatory or prohibited, but there is something empty about 
it. Acting on this maxim could never result in any action. If an agent adopts this maxim, 
they decide that, in the circumstances ``eating breakfast'' they will perform the act ``eating breakfast''
for the purpose ``eating breakfast.'' In these circumstances, the act has 
already been performed! Adopting this maxim as a law to live by does not change how you live. If you adopt 
this maxim, when you are eating breakfast, you eat breakfast, but this statement is already tautologically true. 

Not only does a badly-formed maxim fail to prescribe action, any obligations or prohibitions it 
generates have already been fulfilled or violated. If a badly-formed maxim generates a prohibition, 
then this prohibition is impossible to obey, which is why my original version of the FUL was inconsistent. 
It is impossible to not eat breakfast while eating breakfast, because the circumstances assume that the 
act has happened. On the other hand, if a badly-formed maxim generates an obligation, then the obligation 
will have already been fulfilled. If you are required to eat breakfast while eating breakfast, then you've 
already fulfilled your obligation because the circumstances assume that the act has happened. Thus, 
a badly-formed maxim does not actually guide action because it doesn't generate new obligations or 
prohibitions. 

Because badly-formed maxims can't prescribe or alter action, they are not practically action-guiding and 
thus are not the right kinds of maxims for practical reasoners to evaluate. Insofar as ethics 
is supposed to guide action, badly-formed maxims cannot be part of this project because they
have no bearing on what someone should do. Practical reason is the kind of reason that helps us decide 
what we should do. A practical reasoner asks moral questions not as a mental puzzle or out of curiosity, but 
in order to decide how to act. A badly-formed maxim can never be action-guiding because it prescribes no new
actions or obligations. It is not the kind of maxim that a practical reasoner should consider, because it
will have no bearing on what the agent should do. There is no explicit prohibition against a badly-formed maxim 
like the breakfast example above, but it is the wrong kind of question for a practical reasoner to ask. 

Kantians can make an even stronger claim about badly-formed maxims—because maxims are laws that you 
give to yourself, asking if you should will a maxim as you will it undermines your will's law-giving 
ability. The circumstances of a badly-formed maxim assume that the agent has willed the maxim. Under 
the Kantian acount of willing, willing a maxim is equivalent to giving the maxim to yourself as a law. 
When you will a maxim, you commit yourself to the maxim's end. You cannot simultaneously 
commit yourself to a maxim and ask if you should be committing to it. To will the maxim is to adopt it as 
law—so the question, ``should I be willing this?'' is paradoxical. Either you haven't actually made 
the maxim your law (and thus haven't yet committed yourself to it), or you aren't actually asking 
the question (because the decision has already been made). Because a maxim is a law that you give to 
yourself, you cannot question it absent a sufficient reason (such as a change in the circumstances). 
To question a law arbitrarily is to not regard it as a law at all. This kind of questioning amounts to 
questioning the will's authority over itself, but this is impossible. The will definitionally has authority 
over itself, for that is what it is to be a will. 

A skeptic may argue that we do often ask ``should I be doing this?'' as we do something. 
Can this kind of question ever be valid? To understand this worry, I consider the maxim, 
``When dancing, I should just dance for the sake of dancing.'' While this maxim appears to be badly-formed (the 
circumstance ``dancing'' implies the act and goal of dancing), it is a question that practical reasoners 
do ask. I argue that the correct interpretation of this maxim is no longer a badly-formed maxim.

Under one reading of this maxim, ``I should just dance'' is referring to a different act than the 
circumstance ``when dancing''. The circumstance ``when dancing'' refers 
to rythmically moving your body to music, but ``I should just dance'' refers to dancing without anxiety, 
completely focused on the joy of dancing itself. More precisely, this maxim should read ``When 
dancing, I should abandon my anxiety and focus on dancing for the sake of dancing.'' This maxim when so 
modified is not vacuous at all—abandoning anxiety and focusing on dancing is an entirely different act 
from moving your body rythmically to music. This maxim is actually well-formed, and thus doesn't
pose a problem for my argument. It is entirely plausible to tell yourself ``When I am dancing, I should focus 
on dancing for the sake of dancing itself.'' The circumstances do not entail the act or the goal because 
they refer to different meanings of the word dancing. Any valid reading of this maxim will have the structure above, 
in which the act is actually different from the circumstances. A reasoner cannot accept their will 
as law-giving or commit themselves to an act and simultaneously question the act. Either they must be 
questioning a different act or they must have recieved new information to prompt the questioning, 
modifying the circumstances of the original maxim. 

Another related worry has to with maxims that we think are prohibited. Consider the maxim modified to 
read ``When dancing and seeing a child drowning, I should dance for the sake of dancing.'' Clearly this 
maxim is fit for moral evaluation, and we expect a moral theory to prohibit this maxim. The circumstances 
``When dancing and seeing a child drowning'' appear to entail the act of dancing, and the maxim thus 
appears badly-formed. Once again, this maxim is formulated incorrectly. In this case, the question 
that the agent is actually asking themselves is ``should I continue dancing?'' That is the 
maxim that they will adopt or reject. They want to know if they should stop dancing and go help the child. 
Dancing at the current moment and dancing at the next moment are different acts, and the circumstances 
imply the former but not the latter. A badly-formed maxim would have circumstances and act both 
``dancing at moment t,'' but this maxim has circumstances ``dancing at moment t'' and act ``dancing 
at moment t+1.''

\noindent \textbf{Implications for Self-Doubt and Self-Respect}

The idea that we cannot even evaluate, let alone will badly formed maxims has implications for 
self-doubt and self-respect. The dancing maxim can be read using the lens of self-doubt. Under this 
reading, the question ``When I am dancing, should I be dancing for the sake of dancing?'' is the agent asking, 
``Am I doing the right thing right now?'' Unlike the drowning example, the agent is not asking about the 
next moment, but is expressing doubt about the moral validity of their behavior at this current moment. 
I do not argue that self-doubt always undermines the will—after all, self-doubt plays an 
important role in moral reasoning and is often the mark of a thoughtful agent. I argue instead 
that questions of self-doubt do not actually involve badly-formed maxims, for these are not the maxims 
that the agent is doubting. This example demonstrates that the tension between self-doubt and self-respect 
arises from a mistaken characterization of questions of self-doubt as questions about badly-formed maxims.
I first explain the tension between self-doubt and self-respect in epistemology, then 
explain the parallel tension in ethics, and finally present a resolution of this tension.

In epistemology, there is a tension between the rational requirement to believe in yourself and the 
value of self-doubt, in moderation. Christensen presents the ``principle of self-respect,'' which requires 
that any rational agent refrain from believing that they have mistaken beliefs \cite[4]{christensen}. For example, I cannot 
rationally both believe that the sky is blue and believe that I believe that the sky is green. In other words, I cannot 
disapprove of my own credences. Christensen argues that this principle, which he abbreviates to SR, holds because 
a perfectly rational agent can make accurate and confident judgements about what they believe. If this 
is the case, violating SR results in a simple contradiction \cite[8-9]{christensen}. 

While most philosophers accept some version of SR\footnote{Christensen cites \citet{vanfraaseen}, 
\citet{vickers}, and \citet{koons}.}, 
Roush argues that the principle must be modified in order to account for healthy epistemic 
self-doubt. She argues that, while pathological second-guessing is correctly criticized, we are generally 
imperfect beings, and some sensitivity to our own limitations is a virtue \cite[2]{roushselfhelp}. Even Christensen 
acknowledges that total self-confidence is an epistemic flaw \cite[1]{christensen}. Thus, there is tension between the rational
requirement to respect our authority as believers and the practical reality that we are often wrong. 

This debate between self-respect and self-doubt in epistemology can be extended to ethics. When we 
commit ourselves to acting, we cannot simultaneously doubt the validity of our action. If human 
behavior is purposive, then the very act of committing implies that one has sufficient reasons for 
committing. These reasons may be flawed, but in making the commitment, the reasoner accepts them. It 
is contradictory to claim that someone commits and questions simultaneously. Either the commitment 
is not real, or the question is not. I will call the principle that one cannot will a maxim and 
simultaneously question if they should will that maxim ``ethical self-respect'' or ESR.

On the other hand, self-doubt is an important part of ethical reasoning. Just as believers are often 
mistaken, so are practical reasoners. An agent who is always sure that they are doing the right thing 
is not thinking deeply enough about their obligations. Some degree of ethical self-doubt is desirable. 
Thus, there is tension between the rational requirement of ESR and the intuitive validity of ethical 
self-doubt (ESD).

To resolve this tension, I return to my earlier example of a dancer. Imagine Sara is dancing at a 
weddding, when, in a moment of angst, she asks herself, ``Should I really be dancing right now?'' 
It seems that she asking if the maxim, ``When dancing at your friend's wedding, dance for the sake 
of dancing'' is a permissible maxim to act on. Notice that the maxim in question is badly-formed: the 
circumstance ``when dancing at a friend's wedding'' implies the act ``dance.'' Because this is a 
badly-formed maxim, it cannot be the maxim that she is questioning, for adopting
this maxim could not have changed her behavior at all. Sara is asking a question about her actions and 
their validity. Any conclusions about the validity of a badly-formed maxim would not help her, first because 
the maxim has no effect on her action, and second because any such validity would be a foregone conclusion
since she has already adopted the maxim. Thus, under the interpretation of self-doubt as a badly-formed 
maxim, the tension between ESR and ethical self-doubt seems irresolvable. Those committed to this 
interpretation must abandon one principle or the other.

To resolve this issue, I turn to another interpretation of ethical self-doubt. Under this interpretation, 
when Sara asks, ``Should I really be dancing right now?'' she wants to know if the maxim that 
resulted in the current moment when she is on the dance floor was the right thing to will. She is 
asking if she made the right decision in the past, when she decided to dance. The maxim that initiated 
the dancing may be something like ``When at a wedding, dance for the sake of dancing.'' This is the maxim 
that she is currently acting on, not the badly-formed maxim  from above. Under this interpretation, 
there is no tension at all between self-doubt and self-respect. It is perfectly valid for a reasoner 
to doubt their prior moral judgements, just as it is perfectly rational for a believer to doubt their 
past beliefs \cite[3-4]{christensen}. Such doubt does not undermine the reasoner's decision-making 
capacity and is thus perfectly consistent with ethical self-respect. 

The tension between ESR and ESD arises from a misreading of questions of self-doubt as questions about 
the evaluation of badly-formed maxims. A question of self-doubt cannot refer to a badly-formed maxim and must 
instead refer to a well-formed maxim about the agent's past decision-making. As seen before, cases where 
agents appear to ask themselves about badly-formed maxims are mistaken about the maxim in question, because 
such a question could never yield a useful answer for a practical reasoner.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsubsection{An Argument For Computational Ethics%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
The philosophical insight above is an example of the kind of philosophical progress that can be 
made using computational tools and serves as evidence for the power of computational ethics. The 
idea that the FUL can only hold for well-formed maxims would have been
incredibly difficult to discover without a computer. I discovered it while formulating the FUL because 
Isabelle's proof-finding tools look for edge cases like badly-formed maxims. Badly-formed maxims are 
interesting because they are the kind of thing that is usually ignored in ordinary philosophical inquiry. 
Philosophers usually assume that we are not discussing badly-formed maxims because, as argued above, 
they are not the kind of thing that is usually useful to study. Computational tools like Isabelle
require that assumptions like the exclusion of well-formed maxims are made precise, and thus force 
philosophers to understand their arguments in a new way. This example demonstrates the contributions
that computational ethics makes: it can quickly check edge cases and it requires that all assumptions be
made explicit and precise.

I do not argue that computational ethics uncovers philosophical insights that humans are incapable 
of reaching. Instead, I claim that insights like the one about the well-formed maxim are much easier
to reach with computational tools. Badly-formed maxims are usually ignored in philosophical discussion 
and would have been much more difficult to understand without the help of a computer which immediately
noted that they render the FUL inconsistent. Computational tools prompt philosophers to ask new questions that 
lead to insights. Computational ethics can serve as another tool in a philosopher's arsenal, like a 
thought experiment or counterexample.

The first benefit of computational ethics is precision, which is the goal of much analytic
philosophy. Thought experiments, arguments, counterexamples, and examples 
illustrate features of a concept in the hope of making the concept itself more precise. Computational 
ethics can help philosophers reach the goal of precision. Representing a philosophical idea in logic 
and implementing it in an interactive theorem prover requires making the idea precise to a degree 
that ordinary discussion does not necessarily require. For example, as I formalized the notion of a 
maxim, I had to understand its components and define it as a circumstance, act, goal tuple. Moreover, 
Isabelle's strict typing system required that I define coherent, consistent types for each of these 
entities and for a maxim as a whole. This precision is possible without computational tools, but 
computational ethics forces a level of precision that ordinary discussion does not demand. Type 
fuzziness and overloaded definitions are all too common in philosophical writing and 
discussion, but computers diaallow this kind of imprecision.

Another benefit of computational ethics is that it makes certain kinds of ethical inquiry, such as 
searching for counterexamples or formal ethics, far less tedious. For example, Nitpick can refute 
an ethical statement in seconds by using brute force to construct a counterexample, something that can require hours
of thought and discussion. I arrived at the insight about badly-formed maxims because Isabelle 
can check edge cases, like that of the badly-formed maxim, far more quickly than a human being. Moreover, 
subfields that use symbolic logic to represent philosophical concepts (e.g. philosophy of language) can 
use interactive theorem provers like Isabelle to complete proofs in a matter of seconds. By automating 
away the tedium, computational ethics can give philosophers the tools to ask new kinds of questions.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
Computational ethics is at its infancy. The use of theorem provers in mathematics is just now beginning 
to make headway \citep{buzzardvideo}, even though theorem provers were first invented in the 1960's \citep{historyofITP}. 
In contrast, the first attempts to use theorem provers for ethics occurred in the last decade. The 
fact that this nascent technology is already helping humans reach non-trivial philosophical conclusions 
is reason to, at the very least, entertain the possibility of a future where computational ethics 
becomes as normal for philosophers as using a thought experiment.

To the skeptic, the fact that a theorem prover requires specialized knowledge outside of the field 
of philosophy indicates that the technology is nowhere near ready for universal use in philosophy 
departments. However, history indicates that as computing power increases and computer scientists make 
progress, computational ethics will become more usable. Theorem provers in mathematics began as toys 
incapable of proving that the real number 2 is not equal to the real number 1, but 
moving from basic algebra to Fields medal winning mathematics became possible in a
matter of years \citep{buzzardvideo}. Countless examples from the history of computer science, from the Turing 
Test to AI game playing to protein folding, demonstrate that progress in computer science can make seemingly 
obscure computer programs useful and usable in ways that exceed our wildest imaginations.
Programmable computers themselves initially began as unwieldy punch card readers, but their current ubiquity 
need not be stated. If computer scientists and philosophers invest in computational ethics, it can 
become as much a tool for philosophy as a calculator is for for arithmetic.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsection{Theoretical Objections to Automating Kantian Ethics%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
Many philosophers cringe at the idea that a computer could perform ethical reasoning or that the 
categorical imperative could provide an algorithm for moral judgement. For example, Rawls asserts, 
``it is a serious misconception to think of the CI-procedure as an algorithm intended to yield, 
more or less mechanically, a correct judgment. There is no such algorithm, and Kant knows this'' \citep[166]{rawlslectures}. 
Ebels-Duggan also claims, ``no one supposes that the Categorical Imperative provides a mechanical 
algorithm that delivers all by itself a complete account of what we ought to do in any given 
situation'' \citep[174]{ebelsduggan}. However unmechanical ethical reasoning
may seem, these claims are not obvious and require further justification. Philosophers who believe 
that mental activity completely determines moral reasoning must explain why computers can, in theory, 
simulate certain mental processes like arithmetic and language, but cannot perform ethical reasoning. 
Without a soul or God-based account of ethical reasoning, it is not obvious that it is theoretically 
impossible to automate ethical reasoning. After all, computers may eventually learn to simulate human 
mental activity entirely, as shown by progress in brain simulation \citep{brainsimulation}. 

In this section, I explore potential arguments for why the categorical imperative could not be automated. When 
philosophers say that the categorical imperative is not an algorithm, they are often are gesturing to the complexity
of ethical judgement. They refer to the difficulty in determining morally relevant circumstances of a maxim or the common 
sense required for a computer to behave ethically as arguments against a categorical imperative ``algorithm.'' 
I will show in this section that these difficulties do not render automated Kantian ethics impossible, but 
merely difficult. There categorical imperative may not provide a simple and immediate algorithm, but, as
I demonstrate in this thesis, some parts of moral judgement using the FUL can be automated using not one
algorithm, but the many algorithms necessary to automatically prove logical theorems.

In \emph{Universal Laws and Ends In Themselves}, O'Neill argues against the existence of an algorithm for
moral behavior. She points out that Kant draws an important distinction between a morally worthy maxim
and a morally worthy action: the latter requires a good will, or a will motivated by duty. She argues that, 
``Kant defines duty not (as would be common today) as outward performance of a certain sort, but as action
that embodies a good will'' \citep[345]{oneilluniversallaws}. Moral behavior doesn't just require performing
a ``good'' action, but it requires acting on a morally worthy maxim from the motivation of duty, or 
doing the right thing because it is the right thing to do. Moral behavior requires both a certain action 
(acting on a morally worthy maxim) and a certain motivation (the motivation of duty). It is the capacity 
for self-motivation that makes morality binding for rational beings; we must behave morally precisely 
because we have wills, or the ability to be motivated by ends
that we choose. Only rational beings have wills, thus only rational beings can have good wills, or wills
motivated by duty, and thus only rational beings can behave morally. Under this understanding
of moral behavior, it seems unlikely that a computer could, in the near future, behave morally
since a computer does not have motivation in the same way as a human being. If a computer is not a fully rational
being, then, it is not the kind of thing that can behave morally.\footnote{A parallel argument can also be made for virtue ethics. Virtuous
behavior requires not only a certain action, but also a certain disposition towards the action, so it seems
difficult for an AI agent to truly behave virtuously.}

The idea that, under Kant's account, a computer cannot behave morally, does not preclude the kind 
of automated categorical imperative test that I present in this thesis. O'Neill argues that the FUL
serves as a test of morally worthy maxims, and an automated categorical imperative test can be used 
to identify this kind of maxim. Perhaps a computer cannot act on a morally relevant maxim from a motivation of duty, 
but it certainly can act on this maxim nonetheless. For example, a self-driving car can choose to swerve to hit a tree
to avoid injuring pedestrians in the crosswalk. This action may be one that acts on a morally worthy maxim
\emph{even if} the self-driving car is not motivated by duty. The discpline of machine ethics is partially
spurred by the recognition that, as automated agents become more powerful, they will need to make
morally consequential decisions. Automated agents may be incapable of moral behavior, but automated agents that mimic
moral behavior are surely better than agents that ignore morality entirely. Worries about unethical AI
stem from the fact that AI agents are navigating a world inhabited by human beings, and their decisions
impact us. Insofar as we are building AI that will operate in human society, the behavior of such AI 
should mimic the behavior of an ethical human being. AI needs to be ethical for our sakes, so that we 
can interact with it safely. If AI conforms to human ethics, then it will certainly navigate the world
in a way that benefits human beings.

Another challenge for automated Kantian ethics identified by O'Neill is that the FUL test requires that
a maxim be given as input. O'Neill notes that the test assumes ``that agents will have certain tentative 
plans, proposals and policies which they can consider, revise or reject or endorse and pursue'' \citep[343]{oneilluniversallaws}.
The FUL evaluates the moral worth of a maxim given as input and this potential maxim is generated by 
the choices that an agent is faced with. Determining this potential maxim is a challenge for both human
and automated reasoners. Kant claims that the difficulty of determining an agent's potential maxim, which is their
own, subjective understanding of their principle of action, is a reason that we may never be able 
to know if morally worthy action has been performed \cite[345]{oneilluniversallaws}. Reasoners are faced with
choices between potential actions and must determine the maxim, or principle, underlying each potential action.
This is equivalent to a ``mapping'' problem: agents are given situations or dillemas as input and must map
these to maxims.

The challenge of mapping actions to maxims is a limitation of my system, but it is not insurmountable. In Section \ref{aiethics},
I argued that, before my system can be used in practice, it must be paired with an input parser that can
translate choices that an automated agent faces into maxims in a logic that my system can
evaluate. This need follows from the difficulty in mapping a potential action
to the maxim of action, whether concerning human action or machine action. As argued in Section \ref{AIethics},  
this is not an insurmountable obstacle for automated ethics and heuristic-based approaches could resolve
this issue. Determining the maxim of action is a challenge for Kantian human beings \citep{oneilluniversallaws}, 
so it is unsurprising that it is a major hurdle for automated Kantian agents. 

As one of the strongest arguments against a categorical imperative algorithm, O'Neill argues that 
the FUL is not supposed to provide a mechanism for deriving all morally worthy maxims from scratch. She notes
that ``we usually already have learnt or worked out the moral standing of many common maxims of duty,''
and so approach moral deliberation with an ``almanac'' of morally worthy and empty maxims \citep[394]{oneilluniversallaws}. 
Rational agents navigating the world rarely recalculate the moral status of each potential maxim of 
action; instead, we consult our almanac of maxims. This almanac is generated by moral education, 
absorbed social values, and moral advice from people we trust. The categorical imperative is useful 
to verify the rightness or wrongness of a maxim, but is not part of the bulk of human ethical reasoning.

While human beings cannot repeatedly apply the universalizability test to all potential maxims during 
every moral dilemma, computers have the computational power to do so. Human beings are 
equipped with enough prior knowledge or common sense, to have an almanac of morally worthy maxims,
but we have limited computational power. Computers, on the other hand, are comparatively
much more capable of computation and thus can repeatedly recompute the results of the categorical
imperative test. They do not come equipped with an almanac of maxims, but can simply recompute this
almanac every time they need to make a decision. Human beings use common sense to make up for their computational
limitations, and automated moral agents can use computational power to reduce the need for common sense.

Daniela Tafani takes this argument one step further by arguing that this ``alamnac'' of maxims already 
includes the moral status of the maxims in questions; human beings already
know which maxims are morally worthy and which are morally lacking. The categorical imperative test
merely reminds us, in moments of weakness, when we are tempted to make an exception to the moral law for 
our own convenience or pleasure, that the moral law has no exceptions \citep[9]{tafani}. Thus, she claims
that ``the Kantian test is therefore as useless for machines as it is for anyone who does
not already know what to do'' \citep[8]{tafani}.\footnote{Translated from the original paper using Google Translate.} 
Understanding the categorical imperative test as a reminder
instead of a derivation tool also explains the fact noted in Section \ref{AIethics} that the FUL cannot 
handle bad-faith attempts to generate false positives or negatives. The test only returns the right 
result when an agent sincerely attempts to represent their maxim of action, not when an adversary attempts
 to ``trick'' the categorical imperative.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{figure}
\centering
\includegraphics[scale=0.5]{inputparser.png}
\caption{A refined version of Figure \ref{fig:AIengine} in which the input parser learns from a database
of action-maxim mappings, which is in turn fed the output of my automated Kantian ethics system. } \label{fig:inputparser}
\end{figure}
%
\begin{isamarkuptext}%
This understanding of the role of the categorical imperative not only fails to render automate moral
reasoning impossible, but it also offers insight into how to solve the challenge of creating an input parser.
If the categorical imperative test is only useful to those who have some prior moral knowledge, then prior moral
knowledge can and should be used to create an input parser. Specifically, some kind of machine learning-based approach
could learn action-maxim mappings from a database of such mappings compiled by a human being. Moreover, 
the human being could assign each maxim in the database a rightness or wrongness score. My implementation
of the automated categorical imperative would then simply check the work of this machine learning algorithm and transform
a fuzzy prediction into a provable, rigorous moral judgement. Moreover, this rigorous moral judgement
could in turn be fed into the database of maxims to make the intput parser smarter. One example of 
this kind of system is shown in Figure \ref{fig:inputparser}. The combination of 
prior knowledge of some maxims' moral worth and the ability of a computer to constantly perform the
universalizability test could not only match human ethical reasoning but perhaps also surpass it
by double checking the moral intuitions that we take for granted. A computer with no common sense or prior knowledge
may indeed be unable to reason using the categorical imperative, but one equipped with some prior knowledge
of maxims and their moral worth may even help us better reason about morality.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsection{Related Work \label{relatedwork}%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
In 1685, Leibniz dreamed of a calculator that could resolve philosophical and theological 
disputes \cite{leibniz}. At the time, the logical and computational resources necessary to make his 
dream a reality did not exist. Today, automated ethics is a growing field, spurred in part by the 
need for ethically intelligent AI agents. Tolmeijer et al. surveyed the state of the field of 
machine ethics \citep{mesurvey} and characterized implementations in automated ethics by (1) the choice 
of ethical theory, (2) implementation design decisions (e.g. logic programming), and (3) implementation 
details (e.g. choice of logic). 

Two branches of automated ethics are top-down and bottom-up ethics. Top-down automated ethics begins 
with an ethical theory, whereas bottom-up automated ethics learns ethical judgements from prior 
judgements. One example of bottom-up automated ethics is Delphi, which uses deep learning to make 
ethical judgements based on a dataset of human judgements \citep{delphi}. While Delphi displays great 
flexibility, it often produces contradictory judgements, such as claiming that taxing exploitative 
profitable companies is good, but burdening successful companies with high tax rates is bad \citep{verge}. 
Because Delphi draws on error-prone human judgements instead of philosophical literature, it makes 
the same judgement errors that humans make. Moreover, because Delphi used a bottom-up approach, 
there is no explicit ethical theory explaining its judgements, so analytically arguing for or 
against its conclusions is impossible. Top-down approaches, on the other hand, must be explicit about 
the underlying ethical theories, and are thus more explainable. 

In this paper, I use a top-down approach to formalize Kantian ethics. There is a long line of work 
automating other ethical theories, like consequentialism \citep{util1, util2} or particularism 
\citep{particularism1, particularism2}. I choose to implement Kantian ethics because, as argued in 
Section \ref{whykant}, it is the most formal and least data-intensive of the three major ethical 
traditions. Kantian ethics is a deontological, or rule based ethic, and there is prior work 
implementing other deontological theories \citep{dde, deon1, deon2}. 

Kantian ethics specifically appears to be an intuitive candidate for formalization and implementation 
and there has been both theoretical and practical work on automating Kantian ethics \citep{powers, lin}. 
In 2006, Powers \citep{powers} argued that implementing Kantian ethics presented technical challenges, 
such as automation of a non-monotonic logic, and philosophical challenges, like a definition of the 
categorical imperative. I address the former through my use of Dyadic Deontic Logic, which allows 
obligations to be retracted as context changes, and the latter through my use of the practical 
contradiction interpretation. There has also been prior work in formalizing Kantian metaphysics 
using I/O logic \citep{io}. Deontic logic, which has been implemented in Isabelle/HOL, itself is inspired 
by Kant's ``ought implies can'' principle, but it does not include a robust formalization of the entire 
categorical imperative \citep{cresswell}.

Kroy \citep{kroy} presents a formalization of the first two formulations of the categorical imperative, 
but does not implement it. I implement his formalization of the FUL to compare it to my system. 
Lindner and Bentzen \citep{BL} presented one of the first formalizations and implementations of 
Kant's second formulation of the categorical imperative. They present their goal as ``not to get 
close to a correct interpretation of Kant, but to show that our interpretation of Kant’s ideas can 
contribute to the development of machine ethics.'' My work builds on theirs by formalizing the 
first formulation of the categorical imperative as faithfully as possible. Staying faithful to 
philosophical literature makes my system capable of making robust and reliable judgements. 

The implementation of this paper was inspired by and builds on Benzmüller, Parent, and Farjami's 
foundational work with the LogiKEy framework for machine ethics, which includes their implementation 
of DDL in Isabelle \citep{BFP, logikey}. The LogiKEy project has been used to study metaphysics 
\citep{godel, metaphysics1}, law \citep{constitution}, and ethics \citep{gewirth}, but not 
Kant's categorical imperative.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsection{Conclusion%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\isadelimtheory
%
\endisadelimtheory
%
\isatagtheory
%
\endisatagtheory
{\isafoldtheory}%
%
\isadelimtheory
%
\endisadelimtheory
%
\end{isabellebody}%
\endinput
%:%file=~/Desktop/cs91r/paper/thesis_5_discussion.thy%:%
%:%24=6%:%
%:%28=8%:%
%:%40=10%:%
%:%41=11%:%
%:%42=12%:%
%:%43=13%:%
%:%44=14%:%
%:%45=15%:%
%:%46=16%:%
%:%47=17%:%
%:%48=18%:%
%:%49=19%:%
%:%50=20%:%
%:%51=21%:%
%:%52=22%:%
%:%55=26%:%
%:%56=27%:%
%:%57=28%:%
%:%58=29%:%
%:%59=30%:%
%:%60=31%:%
%:%61=32%:%
%:%64=34%:%
%:%65=35%:%
%:%66=36%:%
%:%67=37%:%
%:%68=38%:%
%:%69=39%:%
%:%73=41%:%
%:%74=42%:%
%:%75=43%:%
%:%76=44%:%
%:%77=45%:%
%:%78=46%:%
%:%79=47%:%
%:%80=48%:%
%:%81=49%:%
%:%82=50%:%
%:%83=51%:%
%:%84=52%:%
%:%85=53%:%
%:%86=54%:%
%:%87=55%:%
%:%88=56%:%
%:%89=57%:%
%:%90=58%:%
%:%91=59%:%
%:%92=60%:%
%:%93=61%:%
%:%94=62%:%
%:%95=63%:%
%:%96=64%:%
%:%97=65%:%
%:%98=66%:%
%:%99=67%:%
%:%100=68%:%
%:%101=69%:%
%:%102=70%:%
%:%103=71%:%
%:%104=72%:%
%:%105=73%:%
%:%106=74%:%
%:%107=75%:%
%:%108=76%:%
%:%109=77%:%
%:%110=78%:%
%:%111=79%:%
%:%112=80%:%
%:%113=81%:%
%:%114=82%:%
%:%115=83%:%
%:%116=84%:%
%:%117=85%:%
%:%118=86%:%
%:%119=87%:%
%:%120=88%:%
%:%121=89%:%
%:%122=90%:%
%:%123=91%:%
%:%124=92%:%
%:%125=93%:%
%:%126=94%:%
%:%127=95%:%
%:%128=96%:%
%:%129=97%:%
%:%130=98%:%
%:%131=99%:%
%:%132=100%:%
%:%133=101%:%
%:%134=102%:%
%:%135=103%:%
%:%136=104%:%
%:%137=105%:%
%:%138=106%:%
%:%139=107%:%
%:%140=108%:%
%:%141=109%:%
%:%142=110%:%
%:%143=111%:%
%:%144=112%:%
%:%145=113%:%
%:%146=114%:%
%:%147=115%:%
%:%148=116%:%
%:%149=117%:%
%:%150=118%:%
%:%151=119%:%
%:%152=120%:%
%:%153=121%:%
%:%154=122%:%
%:%155=123%:%
%:%156=124%:%
%:%157=125%:%
%:%158=126%:%
%:%159=127%:%
%:%160=128%:%
%:%161=129%:%
%:%162=130%:%
%:%163=131%:%
%:%164=132%:%
%:%165=133%:%
%:%166=134%:%
%:%167=135%:%
%:%168=136%:%
%:%169=137%:%
%:%170=138%:%
%:%171=139%:%
%:%172=140%:%
%:%173=141%:%
%:%174=142%:%
%:%175=143%:%
%:%176=144%:%
%:%177=145%:%
%:%178=146%:%
%:%187=148%:%
%:%199=150%:%
%:%200=151%:%
%:%201=152%:%
%:%202=153%:%
%:%203=154%:%
%:%204=155%:%
%:%213=157%:%
%:%225=159%:%
%:%226=160%:%
%:%227=161%:%
%:%228=162%:%
%:%229=163%:%
%:%230=164%:%
%:%231=165%:%
%:%232=166%:%
%:%233=167%:%
%:%234=168%:%
%:%235=169%:%
%:%236=170%:%
%:%237=171%:%
%:%238=172%:%
%:%239=173%:%
%:%240=174%:%
%:%241=175%:%
%:%242=176%:%
%:%243=177%:%
%:%244=178%:%
%:%245=179%:%
%:%246=180%:%
%:%247=181%:%
%:%248=182%:%
%:%249=183%:%
%:%250=184%:%
%:%251=185%:%
%:%252=186%:%
%:%253=187%:%
%:%254=188%:%
%:%255=189%:%
%:%256=190%:%
%:%257=191%:%
%:%258=192%:%
%:%259=193%:%
%:%260=194%:%
%:%261=195%:%
%:%262=196%:%
%:%263=197%:%
%:%264=198%:%
%:%265=199%:%
%:%266=200%:%
%:%267=201%:%
%:%268=202%:%
%:%269=203%:%
%:%270=204%:%
%:%271=205%:%
%:%272=206%:%
%:%273=207%:%
%:%274=208%:%
%:%275=209%:%
%:%276=210%:%
%:%277=211%:%
%:%278=212%:%
%:%279=213%:%
%:%280=214%:%
%:%281=215%:%
%:%282=216%:%
%:%283=217%:%
%:%284=218%:%
%:%285=219%:%
%:%286=220%:%
%:%287=221%:%
%:%288=222%:%
%:%289=223%:%
%:%290=224%:%
%:%291=225%:%
%:%292=226%:%
%:%293=227%:%
%:%294=228%:%
%:%295=229%:%
%:%296=230%:%
%:%297=231%:%
%:%298=232%:%
%:%299=233%:%
%:%300=234%:%
%:%301=235%:%
%:%302=236%:%
%:%303=237%:%
%:%304=238%:%
%:%305=239%:%
%:%306=240%:%
%:%307=241%:%
%:%308=242%:%
%:%309=243%:%
%:%310=244%:%
%:%311=245%:%
%:%312=246%:%
%:%313=247%:%
%:%314=248%:%
%:%315=249%:%
%:%316=250%:%
%:%317=251%:%
%:%318=252%:%
%:%319=253%:%
%:%320=254%:%
%:%321=255%:%
%:%322=256%:%
%:%323=257%:%
%:%324=258%:%
%:%325=259%:%
%:%326=260%:%
%:%327=261%:%
%:%328=262%:%
%:%329=263%:%
%:%330=264%:%
%:%331=265%:%
%:%332=266%:%
%:%333=267%:%
%:%334=268%:%
%:%335=269%:%
%:%336=270%:%
%:%337=271%:%
%:%338=272%:%
%:%339=273%:%
%:%340=274%:%
%:%341=275%:%
%:%342=276%:%
%:%343=277%:%
%:%344=278%:%
%:%345=279%:%
%:%346=280%:%
%:%347=281%:%
%:%348=282%:%
%:%349=283%:%
%:%350=284%:%
%:%351=285%:%
%:%352=286%:%
%:%353=287%:%
%:%354=288%:%
%:%355=289%:%
%:%356=290%:%
%:%357=291%:%
%:%358=292%:%
%:%359=293%:%
%:%360=294%:%
%:%361=295%:%
%:%362=296%:%
%:%363=297%:%
%:%364=298%:%
%:%365=299%:%
%:%366=300%:%
%:%367=301%:%
%:%368=302%:%
%:%369=303%:%
%:%370=304%:%
%:%371=305%:%
%:%372=306%:%
%:%373=307%:%
%:%374=308%:%
%:%375=309%:%
%:%376=310%:%
%:%377=311%:%
%:%378=312%:%
%:%379=313%:%
%:%380=314%:%
%:%381=315%:%
%:%382=316%:%
%:%383=317%:%
%:%384=318%:%
%:%385=319%:%
%:%386=320%:%
%:%387=321%:%
%:%388=322%:%
%:%389=323%:%
%:%398=327%:%
%:%410=329%:%
%:%411=330%:%
%:%412=331%:%
%:%413=332%:%
%:%414=333%:%
%:%415=334%:%
%:%416=335%:%
%:%417=336%:%
%:%418=337%:%
%:%419=338%:%
%:%420=339%:%
%:%421=340%:%
%:%422=341%:%
%:%423=342%:%
%:%424=343%:%
%:%425=344%:%
%:%426=345%:%
%:%427=346%:%
%:%428=347%:%
%:%429=348%:%
%:%430=349%:%
%:%431=350%:%
%:%432=351%:%
%:%433=352%:%
%:%434=353%:%
%:%435=354%:%
%:%436=355%:%
%:%437=356%:%
%:%438=357%:%
%:%439=358%:%
%:%440=359%:%
%:%441=360%:%
%:%442=361%:%
%:%443=362%:%
%:%444=363%:%
%:%445=364%:%
%:%446=365%:%
%:%447=366%:%
%:%448=367%:%
%:%449=368%:%
%:%450=369%:%
%:%451=370%:%
%:%455=374%:%
%:%456=375%:%
%:%457=376%:%
%:%458=377%:%
%:%459=378%:%
%:%460=379%:%
%:%461=380%:%
%:%462=381%:%
%:%463=382%:%
%:%464=383%:%
%:%465=384%:%
%:%466=385%:%
%:%467=386%:%
%:%468=387%:%
%:%469=388%:%
%:%470=389%:%
%:%471=390%:%
%:%472=391%:%
%:%473=392%:%
%:%482=395%:%
%:%494=398%:%
%:%495=399%:%
%:%496=400%:%
%:%497=401%:%
%:%498=402%:%
%:%499=403%:%
%:%500=404%:%
%:%501=405%:%
%:%502=406%:%
%:%503=407%:%
%:%504=408%:%
%:%505=409%:%
%:%506=410%:%
%:%507=411%:%
%:%508=412%:%
%:%509=413%:%
%:%510=414%:%
%:%511=415%:%
%:%512=416%:%
%:%513=417%:%
%:%514=418%:%
%:%515=419%:%
%:%516=420%:%
%:%517=421%:%
%:%518=422%:%
%:%519=423%:%
%:%520=424%:%
%:%521=425%:%
%:%522=426%:%
%:%523=427%:%
%:%524=428%:%
%:%525=429%:%
%:%526=430%:%
%:%527=431%:%
%:%528=432%:%
%:%529=433%:%
%:%530=434%:%
%:%531=435%:%
%:%532=436%:%
%:%533=437%:%
%:%534=438%:%
%:%535=439%:%
%:%536=440%:%
%:%537=441%:%
%:%538=442%:%
%:%539=443%:%
%:%540=444%:%
%:%541=445%:%
%:%542=446%:%
%:%543=447%:%
%:%544=448%:%
%:%545=449%:%
%:%546=450%:%
%:%547=451%:%
%:%548=452%:%
%:%549=453%:%
%:%550=454%:%
%:%551=455%:%
%:%552=456%:%
%:%553=457%:%
%:%554=458%:%
%:%555=459%:%
%:%556=460%:%
%:%557=461%:%
%:%558=462%:%
%:%559=463%:%
%:%560=464%:%
%:%561=465%:%
%:%562=466%:%
%:%563=467%:%
%:%564=468%:%
%:%565=469%:%
%:%566=470%:%
%:%567=471%:%
%:%568=472%:%
%:%569=473%:%
%:%570=474%:%
%:%571=475%:%
%:%572=476%:%
%:%573=477%:%
%:%574=478%:%
%:%575=479%:%
%:%576=480%:%
%:%577=481%:%
%:%578=482%:%
%:%579=483%:%
%:%580=484%:%
%:%581=485%:%
%:%582=486%:%
%:%583=487%:%
%:%584=488%:%
%:%585=489%:%
%:%586=490%:%
%:%587=491%:%
%:%588=492%:%
%:%589=493%:%
%:%590=494%:%
%:%591=495%:%
%:%592=496%:%
%:%593=497%:%
%:%594=498%:%
%:%595=499%:%
%:%596=500%:%
%:%597=501%:%
%:%598=502%:%
%:%599=503%:%
%:%600=504%:%
%:%601=505%:%
%:%604=508%:%
%:%605=509%:%
%:%606=510%:%
%:%607=511%:%
%:%608=512%:%
%:%609=513%:%
%:%612=515%:%
%:%613=516%:%
%:%614=517%:%
%:%615=518%:%
%:%616=519%:%
%:%617=520%:%
%:%618=521%:%
%:%619=522%:%
%:%620=523%:%
%:%621=524%:%
%:%622=525%:%
%:%623=526%:%
%:%624=527%:%
%:%625=528%:%
%:%626=529%:%
%:%635=533%:%
%:%647=535%:%
%:%648=536%:%
%:%649=537%:%
%:%650=538%:%
%:%651=539%:%
%:%652=540%:%
%:%653=541%:%
%:%654=542%:%
%:%655=543%:%
%:%656=544%:%
%:%657=545%:%
%:%658=546%:%
%:%659=547%:%
%:%660=548%:%
%:%661=549%:%
%:%662=550%:%
%:%663=551%:%
%:%664=552%:%
%:%665=553%:%
%:%666=554%:%
%:%667=555%:%
%:%668=556%:%
%:%669=557%:%
%:%670=558%:%
%:%671=559%:%
%:%672=560%:%
%:%673=561%:%
%:%674=562%:%
%:%675=563%:%
%:%676=564%:%
%:%677=565%:%
%:%678=566%:%
%:%679=567%:%
%:%680=568%:%
%:%681=569%:%
%:%682=570%:%
%:%683=571%:%
%:%684=572%:%
%:%685=573%:%
%:%686=574%:%
%:%687=575%:%
%:%688=576%:%
%:%689=577%:%
%:%690=578%:%
%:%691=579%:%
%:%692=580%:%
%:%693=581%:%
%:%694=582%:%
%:%695=583%:%
%:%696=584%:%
%:%697=585%:%
%:%698=586%:%
%:%707=588%:%