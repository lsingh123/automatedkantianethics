%
\begin{isabellebody}%
\setisabellecontext{whykant}%
%
\isadelimtheory
%
\endisadelimtheory
%
\isatagtheory
%
\endisatagtheory
{\isafoldtheory}%
%
\isadelimtheory
%
\endisadelimtheory
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsection{Why Kantian Ethics%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
In this thesis, I automate Kantian ethics. In 2006, Powers posited that deontological theories are 
attractive candidates for automation because rules are generally computationally tractable \cite[1]{powers}. 
Intuitively, algorithms are rules or procedures for problem solving and Kantian ethics offer one such 
procedure for the problem of making ethical judgements. I will make this intuition precise by
arguing that Kantian ethics is natural to formalize because evaluating a maxim requires little additional
data about the world and a maxim is relatively easy to represent to a computer. All 
ethical traditions have debates that an automated ethical system will need to take a stance on, but these debates
are less frequent and less controversial for Kantian ethics than for consequentialism and 
virtue ethics. 

I do not aim to show that Kantian ethics is the only tractable theory to automate and
to present a comprehensive overview of all consequentialist or virtue ethical theories. Instead, I 
present a sample of some approaches in each tradition and argue that Kantian ethics is more straightforward 
to formalize than these approaches. Future work could and should address the challenges I outline in 
this section. The more ethical theories that computational tools can handle, the more valuable 
computational philosophy becomes both for philosophers and for AI agents. Insofar as my project serves 
as an early proof-of-concept for computational ethics, I choose to automate an ethical theory that 
poses fewer challenges than others.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsubsection{Kantian Ethics%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
\textbf{Crash Course on Kantian Ethics}%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
Kant's theory is centered on practical reason, which is the kind of reason that we 
use to decide what to do. In \emph{The Groundwork of the Metaphysics of Morals}, Kant's most influential 
text on ethics, he explains that rational beings are unique because we can act ``in accordance with 
the representations of laws" \cite[4:412]{groundwork}. In contrast, a ball thrown into the air acts 
according to the laws of physics themselves. It cannot ask itself, ``Should I fall back to the ground?" 
It simply falls. A rational being on the other hand, can ask themselves ``Should I act on this reason?" 
As Korsgaard describes it, when choosing which desire to act on, ``it is as if there is something over 
and above all of your desires, something which is you, and which chooses which desire to act on" \cite[100]{sources}. 
Rational beings are set apart by this reflective capacity. A rational being's behavior is purposive and 
their actions are guided by practical reason. They have reasons for acting, even when these reasons may be 
opaque to them. This operation of practical reason is what Kant calls the will. 

The will operates by adopting, or willing, maxims, which are its percieved reasons for acting. Kant defines a maxim as 
the ``subjective principle of willing," or the reason that the will \emph{subjectively} gives 
to itself for acting \cite[16 footnote 1]{groundwork}. There is debate about what exactly must be 
included in a maxim, but most philosophers agree that a maxim consists of some combination of circumstances, 
act, and goal.\footnote{For more discussion of the definition of a maxim, see Section What Is a Maxim}
One example of a maxim is ``when I am hungry, I will eat a doughnut in order to satisfy my sweet tooth." 
When an agent wills this maxim, they decide to act on it. They commit themselves to the end in the maxim 
(e.g. satisfying your sweet tooth). They represent their action, to themselves, as following the 
principle given by this maxim. Because a maxim captures an agent's principle of action, Kant evaluates
maxims are obligatory or prohibited. He argues that certain maxims have a form or logical structure 
that requires any rational agent to will them, and these maxims are obligatory. 

The form of an obligatory maxim is given by the categorical imperative, or the supreme rule of morality. 
An imperative is a command, such as ``Close the door" or "Eat the doughnut in order to satisfy your 
sweet tooth." An imperative is categorical if it holds unconditionally for all rational agents under all 
circumstances. Kant argues that the moral law must be a categorical imperative, for otherwise it would 
not have the force that makes it a moral law \cite[5]{groundwork}. In order for an imperative to be 
categorical, it must be derived from the will's authority over itself. Our wills are autonomous, so 
the only thing that can have unconditional authority over a rational will is 
the rational will itself. In Velleman's version of this argument, he claims that no one else can tell you what 
to do because you can always ask why you 
should obey their authority. The only authority that you cannot question is the authority of your own 
practical reason. To question this authority is to demand a reason for acting for reasons, which 
concedes the authority of reason itself \cite[23]{velleman}. Therefore, the only possible candidates 
for the categorical imperative are those rules that are required of the will because it is a will. 
The categorical imperative must be a property of practical reason itself.

Armed with this understanding of practical reason, Kant presents the categorical 
imperative. He presents three ``formulations" or versions of the categorical imperative and goes on to 
argue that all three formulations are equivalent. In this project, I focus on the first formulation,
the Formula of Universal Law, but will briefly present the other two as well\footnote{For more on this 
choice, see Section Why FUL}. 

The first formulation of the categorical imperative is the
Formula of Universal Law (FUL), which reads, ``act only according to that maxim through which you can 
at the same time will that it become a universal law" \cite[34]{groundwork}. This formulation
generates the universalizability test, which tests the moral value of a maxim by 
imagining a world in which it becomes a universal law and attempting to will the maxim in that world.
If there is a contradiction in willing the maxim in a world in which everyone universally wills the maxim,
the maxim is prohibited. 
Velleman presents a concise argument for the FUL. He argues that reason is universally shared among reasoners. For 
example, all reasoners have equal access to the arithmetic logic that shows that ``2+2=4" \cite[29]{velleman}. The chain of 
reasoning that makes this statement true is not specific to any person, but is universal across people. 
Therefore, if I have sufficient reason to will a maxim, so does every other rational agent. There is 
nothing special about the operation of my practical reason that other reasoners don't have access to. 
Practical reason is shared, so in adopting a maxim, I implicitly state that all reasoners
across time also have reason to adopt that maxim. Therefore, because I act on reasons, I must obey the 
FUL. Notice that this fulfills the above criterion for a categorical imperative: the FUL is derived from 
a property of practical reason itself and thus derives authority from the will's authority over itself, 
as opposed to some external authority.

The second formulation of the categorical imperative is the formula of humanity (FUH): ``So act that you use humanity, 
in your own person, as well as in the person of any other, always at the same time as an end, never merely 
as a means." \cite[41]{groundwork}. This formulation is often understood as requiring us to 
acknowledge and respect the dignity of every other person. The third formulation of the categorical 
imperative is the formula of autonomy (FOA), which Korsgaard summarizes in her introduction to the Groundwork 
as, ``we should so act that we may think of ourselves as legislating universal laws through our 
maxims" \cite[28]{korsgaardintro}. While closely related to the FUL, the FOA presents morality as the activity of 
perfectly rational agents in an ideal ``kingdom of ends," guided by what Kant calls the ``laws of freedom."

The above is not meant to serve as a full defense of Kant's ethical theory, as that is outside the scope
of this thesis. Instead, I briefly reconstruct an argument for Kant's ethical theory in the hopes 
of offering context for the implementation of the FUL I present later in the thesis. Additionally, understanding 
the structure of Kant's argument also reveals facts about his theory that make it an ideal candidate 
for formalization.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
\textbf{Ease of Automation}%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
Kantian ethics is natural to formalize because the categorical imperative, particularly the FUL, 
is a formal principle of practical reason. In other words, the FUL is a property of reason related 
to the form or structure of a maxim. It has nothing to do with the circumstances of behavior 
(beyond those included in a maxim), the agent's mental state, or any other contingent facts. Instead, 
it is purely a property of the proposed principle for action. This formalism makes Kantian ethics an 
attractive candidate for formalization. While other ethical theories often rely on many facts about 
the world or the actor, Kantian ethics simply relies on the form of a given maxim. A computer evaluating 
a maxim doesn't require any knowledge about the world beyond what is contained in a maxim. A maxim 
is the only input that the computer needs to make a moral judgement. Automating 
Kantian ethics merely requires making the notion of a maxim precise and representing it to the computer. 
This distinguishes Kantian ethics from consequentialism and virtue ethics, which, as I will argue below, 
require far more knowledge about the world or the agent to reach a moral decision.

Not only does evaluating Kantian ethics only require evaluating a maxim, a maxim itself is an object
with a thin representation for a computer, as contrasted to more complex objects like states of 
affairs or moral character. Later in my project, I argue that a maxim can be represented simply as 
a tuple of circumstances, act, and goal\footnote{For more, see Section What is a Maxim?}. This representation
is simple and efficient, especially when compared to the representation of a causal chain or a state of 
affairs or moral character. A maxim is a principle with a well-defined form, so representing a maxim
to the computer merely requires capturing this form. This property not only reduces the computational complexity
(in terms of time and space) of representing a maxim, it also make the system easier for human reasoners
to interact with. A person crafting an input to a Kantian automated agent needs to reason about relatively
simple units of evaluation, as opposed to the more complex features that consequentialism and virtue
ethics reqire. I will make the comparision to consequentialism and virtue ethics explicit below.

I do not argue that Kantian ethics is the only theory possible to formalize, but instead that Kantian
ethics is relatively easier to formalize. Indeed, like any ethical theory, the Kantian tradition contains 
debates that philosophers disagree on. In this project, I assume stances on debates about
what exactly constitutes the form of a maxim and the correct interpretation of the Formula of 
Universal Law. Those who disagree with my stances will not trust the judgements of my system. Unlike
consequentialism or virtue ethics, these debates are considered close to settled in the Kantian literature 
\cite{ebelsduggan}. Because such debates are fewer and less controversial than those in virtue ethics
or consequentialism, Kantian ethics is relatively easier to formalize.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsubsection{Consequentialism%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
A consequentialist ethical theory is, broadly speaking, any ethical theory that evaluates an action by evaluating 
its consequences. \footnote{There is long debate about what exactly makes an ethical theory consequentialist \cite{consequentialismsep}. 
For this paper, I will focus on theories that place the moral worth of an act in its the consequences.} For example, 
utilitarianism is a form of consequentialism in which the moral action 
is the action that results in the best consequences or produces the most good \cite{utilsep}. The focus
on the consequences of action distinguishes consequentialists from Kantians, who derive the moral worth
of an action from the maxim that is acted on. Some debates in the consequentialist tradition include 
which consequences of an action matter, what exactly constitutes a ``good" consequence, and how we can 
aggregate the consequences of an action over all the individuals involved.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
\textbf{Which Consequences Matter}%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
Because consequentialism evaluates the state of affairs following an action, this kind of ethical 
reasoning requires more knowledge
about the state of the world than Kantian ethics. Under a naive version of consequentialism, evaluating 
an action requires perfect knowledge of all consequences following an action. This requires that an 
automated ethical system somehow collect all of the infinite consquences following an action, a likely
impossible task. Moreover, compiling this database of consequences requires 
answering difficult questions about which consequences were actually caused by an action.\footnote
{maybe cite the debate about difficulties in determining causation?}

These challenges also apply to human reasoners, so most consequentialists do not actually adopt the naivie
view that agents need to calculate all the consequences of their actions. Plausible strategies to avoid 
this problem include stopping calculation early because constant calculation paralyzes action or only 
evaluating consequences that the agent could reasonably forsee before acting. Another solution is the 
``proximate cause" approach, which only holds the agent responsible for the immediate consequences of their 
acts, but not for consequences resulting from others' voluntary responses to the agent's original act \cite{consequentialismsep}.

Even without understanding the details of these views, it is clear that they require 
more data than Kantian ethics and scale poorly with the complexity of the act being evaluated. Even 
if we cut off the chain of causal reasoning at some point based on 
one of the rules above, evaluating the consequences of an action is still data-intensive. Even evaluating
the first or immediate cause of an action requires knowledge about the state of the world before
and after an action. Kantian ethics, on the other hand, requires only knowledge about the maxim, not 
knowledge about the state of the world when the maxim is adopted. Consequentialism requires knowledge about 
the situation in which the act is performed and following the act, whereas Kantian ethics merely requires 
knowledge about the act itself. For simple acts, collecting this data may not seem unreasonable, but as acts become
more complex and affect more people, the computational time and space required to calculate and store
their consequences increases. Kantian ethics, on the other hand, does not suffer this scaling
challenge because maxims that affect 1 person and maxims that affect 1 million people share the same
representation.

The fact that consequentialism requires more knowledge about the world makes it more difficult to formalize.
Automated consequentialist ethical systems would need to represent complex states of the world and causal chains
in an efficient manner and reason about them. This both presents a difficult technical challenge and
impedes the usability of such a system. Such a system would need to come equipped with 
a large enough database of knowledge about the world to extrapolate the consequences of an actions and
up-to-date information about the state of the world at the moment of action. Not only does collecting 
and representing this data pose a technical challenge, it also creates a larger ``trusted code base" for 
the automated system. Trusting my Kantian ethical reasoner merely requires trusting the logical implementation
of the categorical imperative and the formulation of a maxim, but trusting a consequentialist ethical 
reasoner requires trusting both the logical machinery that actually evaluates the act and the 
background/situational knowledge that serves as an input to this machinery.

The challenge of understanding and representing the circumstances of action is not unique to consequentialism,
but is particularly acute for consequentialism. Kantian ethicists robustly debate which circumstances 
of an action are ``morally relevant" and should be included in the formulation of the maxim.\footnote{Powers 
\cite{powers} identifies this as a challenge for automating Kantian ethics and briefly sketches 
solutions from O'Neill \citet{constofreason}, Silber \citet{silber}, and Rawls \citet{rawlsconstructivism}. } Those 
using my system will need to automatically or manually formulate maxims, a process that involves 
common-sense reasoning to determine which circumstances, act, and goal tuples the agent is adopting.
\footnote{For more on the challenge of parsing of ethical dilemmas into maxims, see Section AI Ethics}
Because Kantian ethics merely evaluates a single maxim, the surface of this debate is much smaller
than the debate about circumstances and consequences in a consequentialist system. An automated 
consequentialist system must make such judgements about the act itself, the circumstances in which 
it is performed, and the circumstances following the act. All ethical theories relativize
their judgements to the situation in which an act is performed to some extent, but consequentialism
requires far more knowledge about the world than Kantian ethics.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
\textbf{Theory of the Good}%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
Another debate that an automated consequentialist reasoner would need to take a stance on is
the question of what qualifies as a ``good consequence," or what the theory of the good is. Hedonists associate
good with the presence of pleasure and the absence of pain. Preference utiliarians believe that good is 
the satisfaction of desires and is thus derived from individuals' preferences, as opposed to some
sensation of pleasure or pain. Other consequentialists adopt a pluralistic theory of value, under which 
many different kinds of things are good for different reasons. For example, Moore values beauty and truth 
and other pluralists value justice, love, and freedom \cite{moorepe}. Welfare utilitarians value a person's 
welfare and utilitarians of right value states of affairs in which respect for some set of
rights is maximized \cite{consequentialismsep}.

Most of the above theories of good require that a moral reasoner understand complex features about
individuals' preferences, desires, or sensations in order to evaluate a moral action, making automated
consequentialist ethics difficult. Regardless of the theory of the good, a consequentialist ethical 
reasoner needs to evaluate a state of affairs, which encompass each involved individual's pleasure, 
preferences, welfare, freedom, rights, or whatever other criteria make a state good. This requires
judgements about whether or not a state of affairs actually satisifes the relevant criteria for goodness. 
These judgements are difficult and debateable, and any consequentialist decision requires many of 
these judgements for each individual involved. As systems become more complex and involve more people and 
more acts, making these judgements quickly becomes difficult, posing a scaling challenge for a 
consequentialist ethical reasoner. Perfect knowledge of tens of thousands of people's pleasure or 
preferences or welfare or rights is impossible. Either a human being 
assigns values to states of affairs, which quickly becomes difficult to scale, or the machine does, 
which requires massive common-sense, increases room for doubting the system's judgements, and simplifies
the judgements. This is a tractable problem, but is much more difficult than the Kantian task of formulating
and evaluating a maxim.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
\textbf{Aggregation}%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
Once an automated consequentialist agent assigns a goodness measurement to each person in a state of affairs, it 
must also calculate an overall goodness measurement for the state of affairs. One approach to assigning
this value is to aggregate each person's individual goodness score into one complete score for a state. 
For example, under a simple welfare model, each person is assigned a welfare score and the total 
score for a state of affairs is the sum of the welfare scores for each involved person.
The more complex the theory of the good, the more difficult this aggregation becomes. For example, 
pluralistic theories struggle to explain how different kinds of value can be compared \cite{consequentialismsep}. 
How do we compare one unit of beauty to one unit of pleasure? Subjective theories of the good, such 
as those focused on the sensation of pleasure or an individual's preferences, present difficulties in 
comparing different people's subjective measures. Resolving this debate requires that the automated reasoner 
choose one specific aggregation algorithm, but those who disagree with this choice will not trust 
the reasoner's moral judgements. Moreover, for complex theories of the good, this aggregation algorithm
may be complex and may require a lot of data. 

To solve this problem, some consequentialists reject aggregation entirely and instead prefer wholistic
evaluations of a state of affairs. While this approach no longer requires that a reasoner define an 
aggregation algorithm, the reasoner still needs to calculate a goodness measurement for a state of 
affairs. Whereas before the reasoner could restrict analysis to a single person, the algorithm must now 
evaluate an entire state wholistically. Evaluating the goodness of an entire state of affairs is more complicated
than evaluating the goodness of a single person. As consequentialists modulate between aggregation 
and wholistic evaluation, they face a tradeoff between the difficulty of aggregation and the complexity 
of goodness measurements for large states of affairs. This tradeoff also holds for an automated
consequentialist moral agent. Such an agent either needs to define an aggregation function, thus opening 
the door to those who disagree with this definition, or needs to evaluate the goodness of entire states
of affairs, which is a complex and data-intensive philosophical and technical challenge.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
\textbf{Prior Attempts to Formalize Consequentialism}%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
None of the challenges described above are intractable or capture the full literature of 
all variations of consequentialism. I do not argue that consequentialism is 
impossible to automate. Instead, each of the challenges above requires that the developer 
``plant certain flags" and take a stance on certain philosophical debates. Such debates are present in 
any ethical theory, but consequentialism has more such points of difficulty than Kantian ethics and 
is thus more difficult to automate. 

Because of its intuitive appeal, automated ethicists have tried to formalize consequentialism in the past.
These efforts cannot escape the debates outlines above. For example, Abel et. al represent ethics as a
Markov Decision Process (MDP), with reward functions customized to particular ethical dilemmas 
\cite[3]{util1}. While this is a convenient representation, it either leaves unanswered or 
takes implicit stances on the debates above. It assumes that consequences can be aggregated just as 
reward is accumulated in an MDP. It leaves open the question of what the reward function is and thus 
leaves the theory of the good, arguably the defining trait of a particular consequentialist view, 
undefined. Similarly, Anderson and Anderson's proposal of a hedonistic act 
utilitarian automated reasoner chooses hedonism\footnote{Recall that hedonism views pleasure as good
and pain as bad.} as the theory of the good \cite[2]{util2}. Again, their proposal assumes that pleasure and pain can be 
given numeric values and that these values can be aggregated with a simple sum, taking an implicit
stance on the aggregation question. Other attempts to automate consequentialist ethics will suffer 
similar problems because, at some point, a useful automated consequentialist moral agent will need 
to resolve the above debates.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimdocument
%
\endisadelimdocument
%
\isatagdocument
%
\isamarkupsubsubsection{Virtue Ethics%
}
\isamarkuptrue%
%
\endisatagdocument
{\isafolddocument}%
%
\isadelimdocument
%
\endisadelimdocument
%
\begin{isamarkuptext}%
\textbf{What Is Virtue}%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
The virtue ethical tradition places the virtues, or those traits that constitute a 
good moral character, at the center. Virtue ethicists evaluate actions based on the character traits 
that such actions would help cultivate. A virtue is commonly accepted as a character trait that 
``makes its possessor good" \cite{vesep}. For example, under Aristotlean virtue ethics, virtues 
are the traits that enable human flourishing or fulfill the purpose of a human being. Many modern 
virtue ethicists abandon Aristotle's notion of a ``purpose" of human beings, and instead define virtue 
in terms of the characteristic activity of human beings (in ethical terms, not telelological terms) \cite{snow}. 
Just as consequentialists must offer a view of which consequences are good, virtue ethicists must offer some 
theory of the virtues which presents and justifices a list of the virtues. Such theories vary
from Aristotle's virtues of courage and temperance to the Buddhist virtue of 
equanimity \cite{aristotle, mcrae}. Another theory is Sen's conception of the virtues as capabilites that create
``effective opportunities to undertake the actions and activities" an agent wants to engage in \cite{robeyns}. 
An automated virtue ethical agent will need to commit to a particular theory of the virtues, opening 
itself up to criticism from those who disagree with this theory of the virtues. Unlike Kantian ethicists, 
who generally agree on the meaning of the FUL, virtue ethicists robustly debate which character traits qualify as virtues.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
\textbf{Evaluating Moral Character}%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
Another difficulty with automating virtue ethics is that the unit of evaluation for a virtue ethical
theory is often a person's entire moral character. While Kantians evaluate the maxim of an act and utilitarians
evaluate the consequences of an act, virtue ethicists evaluate the actor's moral character and their 
disposition towards the act. Virtues are character traits and evaluating an action as virtuous or 
not requires understanding the agent's character and disposition while acting. If states of affairs
require complex representations, an agent's ethical character and disposition is even more difficult
to represent. Consequentialism posed a data-collection problem in evaluating and representing states
of affairs, but virtue ethics poses a conceptual problem about the formal nature of moral character.
Formalizing the concept of character appears to require significant philosophical and computational
progress, whereas Kantian ethics immediately presents a formal rule to implement.%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
\textbf{Machine Learning and Virtue Ethics}%
\end{isamarkuptext}\isamarkuptrue%
%
\begin{isamarkuptext}%
One potential appeal of virtue ethics is that many virtue ethical theories involve some form of 
moral habit, which seems to suggest a machine learning approach. Artistotle, for example, argued 
that cultivating virtuous action requires making such action habitual through moral education \cite{aristotle}. Under 
one view of virtue ethics, the virtuous act is what the virtuous person would do. Both of these ideas
imply that ethical behavior can be learned from some dataset of virtuous acts, either those 
prescribed by a moral teacher or those that a virtuous ideal agent would undertake. Indeed, these 
theories seem to point towards a machine learning approach to computational ethics, in which ethics is 
learned from a dataset of acts tagged as ethical or not ethical. Such approaches exist in the literature, 
as in the Delphi system \cite{delphi}. 

Just as prior work in consequentialism takes implicit or explicit stances on debates in consequentialist
literature, so does work in machine learning-based virtue ethics. For example, the dataset that Delphi uses to learn ethical
behavior contains implicit views on what the virtues are and how certain acts impact an agent's
moral character. Because Delphi lacks a specified, explicit theory of virtue, it is impossible to 
determine which traits the system implicitly classifies as virtues without manually examining the 
dataset. 

Machine learning approaches also may suffer explanability problems that my logical, theorem-prover
based approach does not experience. Many machine learning algorithms cannot sufficiently explain their 
decisions to a human being, and often find patterns or correlations in datasets that don't actually 
cohere with the trends and causes that a human being would identify \cite{puiutta}. While there is significant activity 
and progress in explainable machine learning, interactive theorem provers are designed to be explainable 
at the outset. Indeed, Isabelle can even give the axioms and lemmas it used in constructing a proof, 
allowing a human being to reconstruct the proof independently if they wish. This is not an 
intractable problem for machine learning approaches to computational ethics, but is one reason to 
prefer logical approaches.

Explanability is particularly important in the case of ethics because ethical judgements are often 
controversial and ethics generally requires reflection. Often, the most interesting and important
ethical judgements result from ethical dilemmas. These judgements are usually controversial 
because people's intuitions differ and different theories generate different answers. In these cases,
explainability is particularly important to convince human beings of the correctness of an ethical 
judgement. If a machine tells us to kill one person to save five without justifying this decision, 
acting on this judgement becomes difficult. Second, ethics is a reflective subject. Practical reason 
is the exercise of using reason to decide what to do. Someone who believes an automated reasoner's 
judgements without examining or understanding the reasons for these judgements doesn't seem to be 
doing ethics correctly.\footnote{I make this argument precise in Section Is CE Even Good For Us?} 
This does not preclude other uses of automated ethics, such as automated moral agents or hypothesis 
generation for philosophy, but it does make computer-assisted ethical judgement difficult.%
\end{isamarkuptext}\isamarkuptrue%
%
\isadelimtheory
%
\endisadelimtheory
%
\isatagtheory
%
\endisatagtheory
{\isafoldtheory}%
%
\isadelimtheory
%
\endisadelimtheory
%
\end{isabellebody}%
\endinput
%:%file=~/Desktop/cs91r/paper/whykant.thy%:%
%:%24=6%:%
%:%36=9%:%
%:%37=10%:%
%:%38=11%:%
%:%39=12%:%
%:%40=13%:%
%:%41=14%:%
%:%42=15%:%
%:%43=16%:%
%:%44=17%:%
%:%45=18%:%
%:%46=19%:%
%:%47=20%:%
%:%48=21%:%
%:%49=22%:%
%:%50=23%:%
%:%51=24%:%
%:%52=25%:%
%:%53=26%:%
%:%62=29%:%
%:%74=31%:%
%:%78=33%:%
%:%79=34%:%
%:%80=35%:%
%:%81=36%:%
%:%82=37%:%
%:%83=38%:%
%:%84=39%:%
%:%85=40%:%
%:%86=41%:%
%:%87=42%:%
%:%88=43%:%
%:%89=44%:%
%:%90=45%:%
%:%91=46%:%
%:%92=47%:%
%:%93=48%:%
%:%94=49%:%
%:%95=50%:%
%:%96=51%:%
%:%97=52%:%
%:%98=53%:%
%:%99=54%:%
%:%100=55%:%
%:%101=56%:%
%:%102=57%:%
%:%103=58%:%
%:%104=59%:%
%:%105=60%:%
%:%106=61%:%
%:%107=62%:%
%:%108=63%:%
%:%109=64%:%
%:%110=65%:%
%:%111=66%:%
%:%112=67%:%
%:%113=68%:%
%:%114=69%:%
%:%115=70%:%
%:%116=71%:%
%:%117=72%:%
%:%118=73%:%
%:%119=74%:%
%:%120=75%:%
%:%121=76%:%
%:%122=77%:%
%:%123=78%:%
%:%124=79%:%
%:%125=80%:%
%:%126=81%:%
%:%127=82%:%
%:%128=83%:%
%:%129=84%:%
%:%130=85%:%
%:%131=86%:%
%:%132=87%:%
%:%133=88%:%
%:%134=89%:%
%:%135=90%:%
%:%136=91%:%
%:%137=92%:%
%:%138=93%:%
%:%139=94%:%
%:%140=95%:%
%:%141=96%:%
%:%142=97%:%
%:%143=98%:%
%:%144=99%:%
%:%145=100%:%
%:%146=101%:%
%:%147=102%:%
%:%148=103%:%
%:%149=104%:%
%:%150=105%:%
%:%151=106%:%
%:%152=107%:%
%:%153=108%:%
%:%154=109%:%
%:%158=111%:%
%:%162=113%:%
%:%163=114%:%
%:%164=115%:%
%:%165=116%:%
%:%166=117%:%
%:%167=118%:%
%:%168=119%:%
%:%169=120%:%
%:%170=121%:%
%:%171=122%:%
%:%172=123%:%
%:%173=124%:%
%:%174=125%:%
%:%175=126%:%
%:%176=127%:%
%:%177=128%:%
%:%178=129%:%
%:%179=130%:%
%:%180=131%:%
%:%181=132%:%
%:%182=133%:%
%:%183=134%:%
%:%184=135%:%
%:%185=136%:%
%:%186=137%:%
%:%187=138%:%
%:%188=139%:%
%:%189=140%:%
%:%190=141%:%
%:%191=142%:%
%:%192=143%:%
%:%193=144%:%
%:%194=145%:%
%:%203=148%:%
%:%215=150%:%
%:%216=151%:%
%:%217=152%:%
%:%218=153%:%
%:%219=154%:%
%:%220=155%:%
%:%221=156%:%
%:%222=157%:%
%:%223=158%:%
%:%227=160%:%
%:%231=162%:%
%:%232=163%:%
%:%233=164%:%
%:%234=165%:%
%:%235=166%:%
%:%236=167%:%
%:%237=168%:%
%:%238=169%:%
%:%239=170%:%
%:%240=171%:%
%:%241=172%:%
%:%242=173%:%
%:%243=174%:%
%:%244=175%:%
%:%245=176%:%
%:%246=177%:%
%:%247=178%:%
%:%248=179%:%
%:%249=180%:%
%:%250=181%:%
%:%251=182%:%
%:%252=183%:%
%:%253=184%:%
%:%254=185%:%
%:%255=186%:%
%:%256=187%:%
%:%257=188%:%
%:%258=189%:%
%:%259=190%:%
%:%260=191%:%
%:%261=192%:%
%:%262=193%:%
%:%263=194%:%
%:%264=195%:%
%:%265=196%:%
%:%266=197%:%
%:%267=198%:%
%:%268=199%:%
%:%269=200%:%
%:%270=201%:%
%:%271=202%:%
%:%272=203%:%
%:%273=204%:%
%:%274=205%:%
%:%275=206%:%
%:%276=207%:%
%:%277=208%:%
%:%278=209%:%
%:%279=210%:%
%:%280=211%:%
%:%281=212%:%
%:%282=213%:%
%:%283=214%:%
%:%284=215%:%
%:%285=216%:%
%:%286=217%:%
%:%290=220%:%
%:%294=222%:%
%:%295=223%:%
%:%296=224%:%
%:%297=225%:%
%:%298=226%:%
%:%299=227%:%
%:%300=228%:%
%:%301=229%:%
%:%302=230%:%
%:%303=231%:%
%:%304=232%:%
%:%305=233%:%
%:%306=234%:%
%:%307=235%:%
%:%308=236%:%
%:%309=237%:%
%:%310=238%:%
%:%311=239%:%
%:%312=240%:%
%:%313=241%:%
%:%314=242%:%
%:%315=243%:%
%:%316=244%:%
%:%317=245%:%
%:%318=246%:%
%:%322=248%:%
%:%326=250%:%
%:%327=251%:%
%:%328=252%:%
%:%329=253%:%
%:%330=254%:%
%:%331=255%:%
%:%332=256%:%
%:%333=257%:%
%:%334=258%:%
%:%335=259%:%
%:%336=260%:%
%:%337=261%:%
%:%338=262%:%
%:%339=263%:%
%:%340=264%:%
%:%341=265%:%
%:%342=266%:%
%:%343=267%:%
%:%344=268%:%
%:%345=269%:%
%:%346=270%:%
%:%347=271%:%
%:%348=272%:%
%:%349=273%:%
%:%350=274%:%
%:%354=277%:%
%:%358=279%:%
%:%359=280%:%
%:%360=281%:%
%:%361=282%:%
%:%362=283%:%
%:%363=284%:%
%:%364=285%:%
%:%365=286%:%
%:%366=287%:%
%:%367=288%:%
%:%368=289%:%
%:%369=290%:%
%:%370=291%:%
%:%371=292%:%
%:%372=293%:%
%:%373=294%:%
%:%374=295%:%
%:%375=296%:%
%:%376=297%:%
%:%377=298%:%
%:%378=299%:%
%:%387=302%:%
%:%399=304%:%
%:%403=306%:%
%:%404=307%:%
%:%405=308%:%
%:%406=309%:%
%:%407=310%:%
%:%408=311%:%
%:%409=312%:%
%:%410=313%:%
%:%411=314%:%
%:%412=315%:%
%:%413=316%:%
%:%414=317%:%
%:%415=318%:%
%:%416=319%:%
%:%417=320%:%
%:%421=322%:%
%:%425=324%:%
%:%426=325%:%
%:%427=326%:%
%:%428=327%:%
%:%429=328%:%
%:%430=329%:%
%:%431=330%:%
%:%432=331%:%
%:%433=332%:%
%:%434=333%:%
%:%438=335%:%
%:%442=337%:%
%:%443=338%:%
%:%444=339%:%
%:%445=340%:%
%:%446=341%:%
%:%447=342%:%
%:%448=343%:%
%:%449=344%:%
%:%450=345%:%
%:%451=346%:%
%:%452=347%:%
%:%453=348%:%
%:%454=349%:%
%:%455=350%:%
%:%456=351%:%
%:%457=352%:%
%:%458=353%:%
%:%459=354%:%
%:%460=355%:%
%:%461=356%:%
%:%462=357%:%
%:%463=358%:%
%:%464=359%:%
%:%465=360%:%
%:%466=361%:%
%:%467=362%:%
%:%468=363%:%
%:%469=364%:%
%:%470=365%:%
%:%471=366%:%
%:%472=367%:%
%:%473=368%:%
%:%474=369%:%
%:%475=370%:%
%:%476=371%:%
%:%477=372%:%
%:%478=373%:%
%:%479=374%:%
%:%480=375%:%